{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Imports and Data Loading\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Masking, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set matplotlib font sizes\n",
        "plt.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 18,\n",
        "    'axes.labelsize': 16,\n",
        "    'xtick.labelsize': 14,\n",
        "    'ytick.labelsize': 14,\n",
        "    'legend.fontsize': 14\n",
        "})\n",
        "\n",
        "# Load CSV from Google Drive\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"DataFrame loaded. Shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xVSmw0MTjqdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Preparing Data â€“ Grouping by Pixel\n",
        "\n",
        "non_feature_cols = [\"Pixel_ID\", \"GDD\", \"Label\"]\n",
        "feature_cols = [c for c in df.columns if c not in non_feature_cols]\n",
        "\n",
        "# Group by Pixel_ID and sort each group by GDD\n",
        "X_list = []   # List of 2D arrays, each array is (time_steps, num_features)\n",
        "y_list = []   # Label for each pixel (0 for Infected, 1 for Non-infected)\n",
        "pixel_ids = []  # Store pixel IDs\n",
        "\n",
        "for pixel_id, group in df.groupby(\"Pixel_ID\", sort=False):\n",
        "    group_sorted = group.sort_values(\"GDD\")\n",
        "    label_str = group_sorted[\"Label\"].iloc[0]\n",
        "    label_num = 0 if label_str == \"Infected\" else 1\n",
        "    X_seq = group_sorted[feature_cols].to_numpy(dtype=np.float32)\n",
        "    X_list.append(X_seq)\n",
        "    y_list.append(label_num)\n",
        "    pixel_ids.append(pixel_id)\n",
        "\n",
        "y_list = np.array(y_list, dtype=np.int32)\n",
        "print(\"Number of pixel sequences:\", len(X_list))"
      ],
      "metadata": {
        "id": "Ey2J-Ynwjyu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Stratified Train/Val/Test Split (One Run)\n",
        "\n",
        "def train_val_test_split(X, y, train_ratio=0.65, val_ratio=0.15, test_ratio=0.20):\n",
        "    # First split: train vs. temp (train_ratio vs. remaining)\n",
        "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(1 - train_ratio), random_state=42)\n",
        "    for train_idx, temp_idx in sss1.split(np.zeros(len(y)), y):\n",
        "        pass\n",
        "    # Second split: split temp into validation and test\n",
        "    X_temp = [X[i] for i in temp_idx]\n",
        "    y_temp = y[temp_idx]\n",
        "    relative_val = val_ratio / (val_ratio + test_ratio)\n",
        "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=(1 - relative_val), random_state=42)\n",
        "    for val_idx, test_idx in sss2.split(np.zeros(len(y_temp)), y_temp):\n",
        "        pass\n",
        "    # Map temp indices back to original indices\n",
        "    val_idx = np.array(temp_idx)[val_idx]\n",
        "    test_idx = np.array(temp_idx)[test_idx]\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "tr_idx, va_idx, te_idx = train_val_test_split(X_list, y_list, 0.65, 0.15, 0.20)\n",
        "print(f\"Train: {len(tr_idx)}, Val: {len(va_idx)}, Test: {len(te_idx)}\")"
      ],
      "metadata": {
        "id": "ry-K3pP-j3-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: LSTM Model Definition\n",
        "def build_lstm_model(num_features, lstm_units1=64, lstm_units2=32, dropout_rate=0.2, dense_units=32, learning_rate=1e-3):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input: variable-length sequences with num_features features\n",
        "    model.add(Masking(mask_value=0.0, input_shape=(None, num_features)))\n",
        "\n",
        "    # First LSTM layer with lstm_units1 and return sequences for stacking\n",
        "    model.add(LSTM(lstm_units1, return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Second LSTM layer with lstm_units2\n",
        "    model.add(LSTM(lstm_units2))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Dense layer for further processing\n",
        "    model.add(Dense(dense_units, activation='relu'))\n",
        "\n",
        "    # Final output layer for binary classification with sigmoid activation\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=Adam(learning_rate=learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Test the model summary using the number of features from feature_cols\n",
        "model_test = build_lstm_model(num_features=len(feature_cols))\n",
        "model_test.summary()"
      ],
      "metadata": {
        "id": "SzpWatMYj7UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Training Loop, Model Saving, and Plotting\n",
        "\n",
        "# Directories for saving models and plots\n",
        "model_save_dir = \"/content/drive/MyDrive/Colab Notebooks/models\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "plots_dir = \"/content/drive/MyDrive/Colab Notebooks/plots\"\n",
        "os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "# Extract training, validation, and test sequences and labels\n",
        "X_train_seqs = [X_list[i] for i in tr_idx]\n",
        "y_train = y_list[tr_idx]\n",
        "X_val_seqs = [X_list[i] for i in va_idx]\n",
        "y_val = y_list[va_idx]\n",
        "X_test_seqs = [X_list[i] for i in te_idx]\n",
        "y_test = y_list[te_idx]\n",
        "\n",
        "# Determine the maximum sequence length from the training set\n",
        "max_len_train = max(seq.shape[0] for seq in X_train_seqs)\n",
        "print(f\"Max training sequence length: {max_len_train}\")\n",
        "\n",
        "# Pad sequences (pad with zeros at the end)\n",
        "X_train_padded = pad_sequences(X_train_seqs, maxlen=max_len_train, dtype='float32', padding='post')\n",
        "X_val_padded = pad_sequences(X_val_seqs, maxlen=max_len_train, dtype='float32', padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seqs, maxlen=max_len_train, dtype='float32', padding='post')\n",
        "\n",
        "# Build the model using the architecture\n",
        "model = build_lstm_model(num_features=len(feature_cols), lstm_units1=64, lstm_units2=32, dropout_rate=0.2, dense_units=32, learning_rate=1e-3)\n",
        "\n",
        "# Set up callbacks: ModelCheckpoint to save best model and EarlyStopping\n",
        "model_filename = os.path.join(model_save_dir, \"model_run1.h5\")\n",
        "checkpoint_cb = ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "earlystop_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_padded, y_train,\n",
        "    validation_data=(X_val_padded, y_val),\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[checkpoint_cb, earlystop_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history: Loss and Accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], marker='o', label=\"Train Loss\")\n",
        "plt.plot(history.history['val_loss'], marker='o', label=\"Val Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], marker='o', label=\"Train Accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], marker='o', label=\"Val Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(plots_dir, \"training_run1.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_prob = model.predict(X_test_padded)\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int).reshape(-1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Test Set Metrics:\")\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "-Fq0Qsf7noLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: LSTM Feature Importance\n",
        "\n",
        "def permutation_importance(model, X, y, baseline_score, feature_names, n_repeats=5):\n",
        "    \"\"\"\n",
        "    Compute permutation feature importance.\n",
        "\n",
        "    Parameters:\n",
        "      model: Trained Keras model.\n",
        "      X: Test data (padded sequences), shape (n_samples, time_steps, n_features).\n",
        "      y: True labels for the test set.\n",
        "      baseline_score: Accuracy score of the model on X (without shuffling).\n",
        "      feature_names: List of feature names corresponding to the last axis of X.\n",
        "      n_repeats: Number of times to repeat the shuffling for averaging.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary mapping feature names to the average drop in accuracy.\n",
        "    \"\"\"\n",
        "    importances = {name: [] for name in feature_names}\n",
        "\n",
        "    for _ in range(n_repeats):\n",
        "        for j, feature in enumerate(feature_names):\n",
        "            X_permuted = np.copy(X)\n",
        "            # Permute the entire time-series for feature j across samples\n",
        "            permuted_indices = np.random.permutation(X.shape[0])\n",
        "            X_permuted[:, :, j] = X_permuted[permuted_indices, :, j]\n",
        "            score = model.evaluate(X_permuted, y, verbose=0)[1]  # use accuracy as metric\n",
        "            importance = baseline_score - score\n",
        "            importances[feature].append(importance)\n",
        "\n",
        "    # Average the importance scores over the repeats\n",
        "    avg_importances = {feature: np.mean(imp_list) for feature, imp_list in importances.items()}\n",
        "    return avg_importances\n",
        "\n",
        "# Compute baseline accuracy on test data\n",
        "baseline_score = model.evaluate(X_test_padded, y_test, verbose=0)[1]\n",
        "\n",
        "# Get feature importances (the higher the drop, the more important the feature)\n",
        "feature_importances = permutation_importance(model, X_test_padded, y_test, baseline_score, feature_cols, n_repeats=5)\n",
        "\n",
        "print(\"Permutation Feature Importances (average drop in accuracy):\")\n",
        "for feature, importance in feature_importances.items():\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "hR726karkHdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: KDE Histogram on Most Important Features\n",
        "\n",
        "# Read CSV file\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Filter rows where Adjusted GDD == 0 (correspond to the vegetation density peak stage)\n",
        "df_gdd0 = df[df[\"GDD\"] == 0]\n",
        "\n",
        "# Define the features for which we need KDE plots\n",
        "features = [\"NDMI\", \"CCC\", \"FAPAR\", \"CHL-RED-EDGE\"]\n",
        "\n",
        "# Color mapping (Non-infected = darker, Infected = lighter)\n",
        "color_mapping = {\n",
        "    \"NDMI\": (\"#0D3B66\", \"#66A5AD\"),       # Non-infected: dark blue; Infected: light blue\n",
        "    \"CCC\": (\"#004D40\", \"#26A69A\"),        # Non-infected: dark teal; Infected: light teal\n",
        "    \"FAPAR\": (\"#4A148C\", \"#BA68C8\"),      # Non-infected: dark purple; Infected: light purple\n",
        "    \"CHL-RED-EDGE\": (\"#006400\", \"#90EE90\")  # Non-infected: dark green; Infected: light green\n",
        "}\n",
        "\n",
        "# Loop over each feature and create a KDE plot\n",
        "for feature in features:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # KDE for Non-infected (Healthy) samples, now labeled as \"Non-infected\"\n",
        "    sns.kdeplot(\n",
        "        data=df_gdd0[df_gdd0[\"Label\"] == \"Healthy\"],\n",
        "        x=feature,\n",
        "        bw_adjust=3,\n",
        "        fill=True,\n",
        "        common_norm=False,\n",
        "        alpha=0.7,\n",
        "        linewidth=2,\n",
        "        color=color_mapping[feature][0],\n",
        "        label=\"Non-infected\"\n",
        "    )\n",
        "\n",
        "    # KDE for Infected samples\n",
        "    sns.kdeplot(\n",
        "        data=df_gdd0[df_gdd0[\"Label\"] == \"Infected\"],\n",
        "        x=feature,\n",
        "        bw_adjust=3,\n",
        "        fill=True,\n",
        "        common_norm=False,\n",
        "        alpha=0.7,\n",
        "        linewidth=2,\n",
        "        color=color_mapping[feature][1],\n",
        "        label=\"Infected\"\n",
        "    )\n",
        "\n",
        "    plt.title(f\"KDE Plot for {feature} (Vegetation Density Peak Stage)\", fontsize=20)\n",
        "    plt.xlabel(feature, fontsize=16)\n",
        "    plt.ylabel(\"Density\", fontsize=16)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure with 300 dpi and transparent background\n",
        "    plt.savefig(f\"{feature}_kde.png\", dpi=300, transparent=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "o19V-8cjkO19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}